{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0ae9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"emotion.csv\", index_col=0, nrows=4000)\n",
    "bert_data = data['text']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf404b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3200/3200 [00:00<00:00, 3214.48 examples/s]\n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 3292.18 examples/s]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25108\\2575573714.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-06-24 11:37:50,585] A new study created in memory with name: no-name-a7e69f5f-e135-488b-99d7-44c71a9f4698\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4745, 'grad_norm': 4.742188453674316, 'learning_rate': 7.981384027594306e-06, 'epoch': 2.5}\n",
      "{'train_runtime': 6906.7265, 'train_samples_per_second': 1.39, 'train_steps_per_second': 0.087, 'train_loss': 0.41117128054300944, 'epoch': 3.0}\n",
      "{'eval_accuracy': 0.91125, 'eval_loss': 0.30465707182884216, 'eval_runtime': 177.7326, 'eval_samples_per_second': 4.501, 'eval_steps_per_second': 0.563, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 13:35:57,069] Trial 0 finished with value: 0.91125 and parameters: {'learning_rate': 4.741416254016419e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 0.91125.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4698, 'grad_norm': 4.130002021789551, 'learning_rate': 5.132429074196233e-08, 'epoch': 5.0}\n",
      "{'train_runtime': 10456.0731, 'train_samples_per_second': 1.53, 'train_steps_per_second': 0.048, 'train_loss': 0.46978387451171877, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 16:33:15,981] Trial 1 finished with value: 0.9025 and parameters: {'learning_rate': 2.5662145370981165e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.91125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9025, 'eval_loss': 0.32960519194602966, 'eval_runtime': 161.7014, 'eval_samples_per_second': 4.947, 'eval_steps_per_second': 0.618, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8405, 'grad_norm': 9.125992774963379, 'learning_rate': 1.3622959336384311e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2147, 'grad_norm': 5.580007553100586, 'learning_rate': 3.90615524481205e-06, 'epoch': 2.5}\n",
      "{'train_runtime': 6671.6288, 'train_samples_per_second': 1.439, 'train_steps_per_second': 0.18, 'train_loss': 0.46282588799794516, 'epoch': 3.0}\n",
      "{'eval_accuracy': 0.91125, 'eval_loss': 0.3528992831707001, 'eval_runtime': 183.1384, 'eval_samples_per_second': 4.368, 'eval_steps_per_second': 0.546, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 18:27:33,041] Trial 2 finished with value: 0.91125 and parameters: {'learning_rate': 2.3320329819773432e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 0.91125.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8513, 'grad_norm': 15.55484676361084, 'learning_rate': 8.710598081453367e-06, 'epoch': 1.25}\n",
      "{'train_runtime': 4972.9471, 'train_samples_per_second': 1.287, 'train_steps_per_second': 0.161, 'train_loss': 0.6436433601379394, 'epoch': 2.0}\n",
      "{'eval_accuracy': 0.88, 'eval_loss': 0.37967589497566223, 'eval_runtime': 167.0422, 'eval_samples_per_second': 4.789, 'eval_steps_per_second': 0.599, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 19:53:15,774] Trial 3 finished with value: 0.88 and parameters: {'learning_rate': 2.315109124638769e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 0.91125.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5425, 'grad_norm': 8.587068557739258, 'learning_rate': 1.4855525661837378e-05, 'epoch': 2.5}\n",
      "{'loss': 0.0828, 'grad_norm': 2.076876401901245, 'learning_rate': 2.965174782801872e-08, 'epoch': 5.0}\n",
      "{'train_runtime': 10987.6537, 'train_samples_per_second': 1.456, 'train_steps_per_second': 0.091, 'train_loss': 0.31265400314331054, 'epoch': 5.0}\n",
      "{'eval_accuracy': 0.9225, 'eval_loss': 0.32162874937057495, 'eval_runtime': 173.2008, 'eval_samples_per_second': 4.619, 'eval_steps_per_second': 0.577, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 22:59:20,020] Trial 4 finished with value: 0.9225 and parameters: {'learning_rate': 2.965174782801872e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 16}. Best is trial 4 with value: 0.9225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best BERT hyperparameters: {'learning_rate': 2.965174782801872e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "\n",
    "le = LabelEncoder()\n",
    "# Prepare data (raw text + label encoded)\n",
    "df_bert = pd.DataFrame({'text': bert_data, 'label': le.fit_transform(y)})\n",
    "train_df, test_df = train_test_split(df_bert, test_size=0.2, stratify=df_bert['label'])\n",
    "\n",
    "# HuggingFace Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=128)\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Format for PyTorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Custom objective function (maximize validation accuracy)\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"eval_accuracy\"]\n",
    "\n",
    "# Hyperparameter space\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 5e-5, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n",
    "    }\n",
    "\n",
    "# Redefine Trainer with placeholder args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_hp_output\",\n",
    "    logging_dir=\"./bert_hp_logs\",\n",
    "    save_strategy=\"no\",\n",
    "    disable_tqdm=True\n",
    ")\n",
    "\n",
    "def model_init():\n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=6\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda p: {\n",
    "        \"eval_accuracy\": (p.predictions.argmax(axis=-1) == p.label_ids).mean()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start tuning\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=compute_objective,\n",
    "    n_trials=5\n",
    ")\n",
    "\n",
    "print(\"Best BERT hyperparameters:\", best_trial.hyperparameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
